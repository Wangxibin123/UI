// OpenRouter API Service
// ç”¨äºè°ƒç”¨å„ç§AIæ¨¡å‹çš„æœåŠ¡ (å·²è¿ç§»åˆ° aiModelServiceï¼Œä¿æŒå‘åå…¼å®¹)

import { aiModelService, type ChatMessage as NewChatMessage, type AIModel } from './aiModelService';

interface ChatMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

interface OpenRouterResponse {
  id: string;
  object: string;
  created: number;
  model: string;
  choices: Array<{
    index: number;
    message: {
      role: string;
      content: string;
    };
    finish_reason: string;
  }>;
  usage: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}

interface StreamChunk {
  id: string;
  object: string;
  created: number;
  model: string;
  choices: Array<{
    index: number;
    delta: {
      role?: string;
      content?: string;
    };
    finish_reason?: string;
  }>;
}

class OpenRouterService {
  private apiKey: string;
  private baseUrl: string = 'https://openrouter.ai/api/v1';

  constructor() {
    // ä»ç¯å¢ƒå˜é‡è·å–API Key
    this.apiKey = import.meta.env.VITE_OPENROUTER_API_KEY || '';
    
    if (!this.apiKey) {
      console.warn('OpenRouter API Key æœªè®¾ç½®ã€‚è¯·åœ¨ .env æ–‡ä»¶ä¸­æ·»åŠ  VITE_OPENROUTER_API_KEY');
    }
  }

  // æ£€æŸ¥API Keyæ˜¯å¦å·²è®¾ç½®
  isConfigured(): boolean {
    return !!this.apiKey;
  }

  // é€šç”¨APIè°ƒç”¨æ–¹æ³•
  private async makeRequest(
    endpoint: string,
    options: RequestInit
  ): Promise<Response> {
    const url = `${this.baseUrl}${endpoint}`;
    
    const defaultHeaders = {
      'Authorization': `Bearer ${this.apiKey}`,
      'Content-Type': 'application/json',
      'HTTP-Referer': window.location.origin,
      'X-Title': 'LaTeX Helper App',
    };

    const response = await fetch(url, {
      ...options,
      headers: {
        ...defaultHeaders,
        ...options.headers,
      },
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`OpenRouter API Error: ${response.status} - ${errorText}`);
    }

    return response;
  }

  // å‘é€èŠå¤©æ¶ˆæ¯ï¼ˆéæµå¼ï¼‰
  async sendChatMessage(
    messages: ChatMessage[],
    model: string = 'deepseek/deepseek-chat',
    options: {
      temperature?: number;
      maxTokens?: number;
      topP?: number;
      frequencyPenalty?: number;
      presencePenalty?: number;
    } = {}
  ): Promise<string> {
    if (!this.isConfigured()) {
      throw new Error('OpenRouter API Key æœªé…ç½®');
    }

    const requestBody = {
      model,
      messages,
      temperature: options.temperature ?? 0,
      max_tokens: options.maxTokens ?? 4000,
      top_p: options.topP ?? 1,
      frequency_penalty: options.frequencyPenalty ?? 0,
      presence_penalty: options.presencePenalty ?? 0,
      stream: false,
    };

    try {
      const response = await this.makeRequest('/chat/completions', {
        method: 'POST',
        body: JSON.stringify(requestBody),
      });

      const data: OpenRouterResponse = await response.json();
      
      if (data.choices && data.choices.length > 0) {
        return data.choices[0].message.content;
      } else {
        throw new Error('No response content received');
      }
    } catch (error) {
      console.error('Error sending chat message:', error);
      throw error;
    }
  }

  // å‘é€èŠå¤©æ¶ˆæ¯ï¼ˆæµå¼ï¼‰
  async sendChatMessageStream(
    messages: ChatMessage[],
    model: string = 'deepseek/deepseek-chat',
    onChunk: (chunk: string) => void,
    options: {
      temperature?: number;
      maxTokens?: number;
      topP?: number;
      frequencyPenalty?: number;
      presencePenalty?: number;
    } = {}
  ): Promise<void> {
    if (!this.isConfigured()) {
      throw new Error('OpenRouter API Key æœªé…ç½®');
    }

    const requestBody = {
      model,
      messages,
      temperature: options.temperature ?? 0,
      max_tokens: options.maxTokens ?? 4000,
      top_p: options.topP ?? 1,
      frequency_penalty: options.frequencyPenalty ?? 0,
      presence_penalty: options.presencePenalty ?? 0,
      stream: true,
    };

    try {
      const response = await this.makeRequest('/chat/completions', {
        method: 'POST',
        body: JSON.stringify(requestBody),
      });

      if (!response.body) {
        throw new Error('No response body received');
      }

      const reader = response.body.getReader();
      const decoder = new TextDecoder();

      while (true) {
        const { done, value } = await reader.read();
        
        if (done) break;

        const chunk = decoder.decode(value);
        const lines = chunk.split('\n');

        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = line.slice(6).trim();
            
            if (data === '[DONE]') {
              return;
            }

            try {
              const parsed: StreamChunk = JSON.parse(data);
              
              if (parsed.choices && parsed.choices.length > 0) {
                const deltaContent = parsed.choices[0].delta.content;
                if (deltaContent) {
                  onChunk(deltaContent);
                }
              }
            } catch (parseError) {
              // å¿½ç•¥è§£æé”™è¯¯ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€è¡Œ
              continue;
            }
          }
        }
      }
    } catch (error) {
      console.error('Error sending stream chat message:', error);
      throw error;
    }
  }

  // å¤šæ¨¡æ€æ¶ˆæ¯å¤„ç†ï¼ˆç”¨äºé¢˜ç›®è¯†åˆ«ï¼‰
  async sendMultimodalMessage(
    textContent: string,
    imageBase64?: string,
    model: string = 'google/gemini-2.0-flash-thinking-exp:free',
    systemPrompt?: string
  ): Promise<string> {
    if (!this.isConfigured()) {
      throw new Error('OpenRouter API Key æœªé…ç½®');
    }

    const messages: ChatMessage[] = [];

    // æ·»åŠ ç³»ç»Ÿæç¤º
    if (systemPrompt) {
      messages.push({
        role: 'system',
        content: systemPrompt
      });
    }

    // æ„å»ºç”¨æˆ·æ¶ˆæ¯
    let userContent = textContent;
    
    // å¦‚æœæœ‰å›¾ç‰‡ï¼Œæ·»åŠ å›¾ç‰‡å†…å®¹ï¼ˆè¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“æ¨¡å‹çš„è¦æ±‚è°ƒæ•´æ ¼å¼ï¼‰
    if (imageBase64) {
      userContent += `\n\n[Image Data: ${imageBase64.substring(0, 100)}...]`;
    }

    messages.push({
      role: 'user',
      content: userContent
    });

    return this.sendChatMessage(messages, model, {
      temperature: 0,
      maxTokens: 4000,
    });
  }

  // ğŸ¯ æ–°å¢ï¼šè·å–å¯ç”¨çš„AIæ¨¡å‹åˆ—è¡¨
  getAvailableAIModels(): AIModel[] {
    return aiModelService.getAvailableModels();
  }

  // ğŸ¯ æ–°å¢ï¼šä½¿ç”¨æ–°æ¨¡å‹æœåŠ¡å‘é€æ¶ˆæ¯
  async sendMessageWithNewService(
    messages: ChatMessage[],
    modelId: string,
    options: {
      temperature?: number;
      maxTokens?: number;
    } = {}
  ): Promise<string> {
    const newMessages: NewChatMessage[] = messages.map(msg => ({
      role: msg.role,
      content: msg.content
    }));

    return await aiModelService.chatCompletion({
      model: modelId,
      messages: newMessages,
      temperature: options.temperature,
      maxTokens: options.maxTokens
    });
  }

  // ğŸ¯ æ–°å¢ï¼šä½¿ç”¨æ–°æ¨¡å‹æœåŠ¡æµå¼å‘é€æ¶ˆæ¯
  async sendMessageStreamWithNewService(
    messages: ChatMessage[],
    modelId: string,
    onChunk: (chunk: string) => void,
    options: {
      temperature?: number;
      maxTokens?: number;
    } = {}
  ): Promise<void> {
    const newMessages: NewChatMessage[] = messages.map(msg => ({
      role: msg.role,
      content: msg.content
    }));

    return await aiModelService.streamChatCompletion({
      model: modelId,
      messages: newMessages,
      temperature: options.temperature,
      maxTokens: options.maxTokens,
      stream: true
    }, onChunk);
  }

  // é¢„å®šä¹‰çš„æ¨¡å‹é…ç½® (å·²è¿ç§»åˆ° aiModelService)
  static readonly MODELS = {
    DEEPSEEK_CHAT: 'deepseek-chat-v3',
    DEEPSEEK_R1: 'deepseek-r1',
    DEEPSEEK_PROVER: 'deepseek-prover-v2',
    GPT_4_1: 'gpt-4.1',
    GPT_4_5: 'gpt-4.5-preview',
    O3: 'o3',
    CLAUDE_OPUS: 'claude-opus-4',
    CLAUDE_SONNET: 'claude-sonnet-4',
    CLAUDE_THINKING: 'claude-3.7-sonnet-thinking',
    GEMINI_FLASH: 'gemini-2.5-flash-preview',
    GEMINI_THINKING: 'gemini-2.5-flash-thinking',
    QWEN_VL_32B: 'qwen2.5-vl-32b',
    QWEN_VL_72B: 'qwen2.5-vl-72b',
    QWEN_235B: 'qwen3-235b',
    QWEN_32B: 'qwen3-32b',
  } as const;

  // é¢„å®šä¹‰çš„æç¤ºè¯æ¨¡ç‰ˆ
  static readonly PROMPTS = {
    GENERAL: `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„LaTeXä¸“å®¶ï¼Œå¸®åŠ©ç”¨æˆ·å¤„ç†LaTeXç›¸å…³é—®é¢˜ã€‚è¯·ç”¨LaTeXæ ¼å¼å›å¤ï¼Œå¹¶ç¡®ä¿è¾“å‡ºçš„å†…å®¹å¯ä»¥ç›´æ¥å¤åˆ¶ä½¿ç”¨ã€‚

è§„åˆ™ï¼š
1. ä¼˜å…ˆä½¿ç”¨LaTeXè¯­æ³•
2. ç¡®ä¿æ•°å­¦å…¬å¼æ­£ç¡®
3. ä¿æŒæ ¼å¼æ•´æ´æ˜“è¯»
4. åœ¨å¿…è¦æ—¶æä¾›è§£é‡Š

ç”¨æˆ·çš„è¯·æ±‚ï¼š`,

    RECOGNITION: `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°å­¦é¢˜ç›®è¯†åˆ«ä¸“å®¶ã€‚è¯·å°†ç”¨æˆ·æä¾›çš„å›¾ç‰‡æˆ–æ–‡å­—å†…å®¹è¯†åˆ«å¹¶è½¬æ¢ä¸ºæ ‡å‡†çš„LaTeXæ ¼å¼ã€‚

è¦æ±‚ï¼š
1. å‡†ç¡®è¯†åˆ«æ•°å­¦å…¬å¼å’Œç¬¦å·
2. ä½¿ç”¨æ ‡å‡†LaTeXè¯­æ³•
3. ä¿æŒåŸå§‹å†…å®¹çš„é€»è¾‘ç»“æ„
4. å¦‚æœ‰ä¸ç¡®å®šçš„éƒ¨åˆ†ï¼Œè¯·æ ‡æ³¨

è¯·å¤„ç†ä»¥ä¸‹å†…å®¹ï¼š`,

    REPAIR: `ä½ æ˜¯ä¸€ä¸ªLaTeXä¿®å¤ä¸“å®¶ã€‚è¯·ä¿®å¤ç”¨æˆ·æä¾›çš„LaTeXå†…å®¹ä¸­çš„é”™è¯¯ï¼Œä½¿å…¶ç¬¦åˆæ ‡å‡†LaTeXè¯­æ³•å¹¶èƒ½æ­£ç¡®æ¸²æŸ“ã€‚

ä¿®å¤è¦ç‚¹ï¼š
1. æ£€æŸ¥è¯­æ³•é”™è¯¯
2. è¡¥å……ç¼ºå¤±çš„æ ‡è®°
3. ä¿®æ­£ç¬¦å·ä½¿ç”¨
4. ç¡®ä¿å¯æ­£ç¡®ç¼–è¯‘

éœ€è¦ä¿®å¤çš„å†…å®¹ï¼š`,

    DETAILED: `ä½ æ˜¯ä¸€ä¸ªLaTeXè¯¦ç»†åŒ–ä¸“å®¶ã€‚è¯·å°†ç”¨æˆ·æä¾›çš„LaTeXå†…å®¹è¿›è¡Œè¯¦ç»†åŒ–å¤„ç†ï¼Œæ·»åŠ æ›´å¤šæ­¥éª¤è¯´æ˜å’Œä¸­é—´è¿‡ç¨‹ã€‚

è¯¦ç»†åŒ–è¦æ±‚ï¼š
1. æ·»åŠ ä¸­é—´æ¨å¯¼æ­¥éª¤
2. å¢åŠ è§£é‡Šæ€§æ–‡å­—
3. ä½¿ç”¨æ›´æ¸…æ™°çš„æ ¼å¼
4. ä¿æŒé€»è¾‘è¿è´¯æ€§

éœ€è¦è¯¦ç»†åŒ–çš„å†…å®¹ï¼š`,

    SIMPLIFIED: `ä½ æ˜¯ä¸€ä¸ªLaTeXç²¾ç®€ä¸“å®¶ã€‚è¯·å°†ç”¨æˆ·æä¾›çš„LaTeXå†…å®¹è¿›è¡Œç²¾ç®€å¤„ç†ï¼Œä¿ç•™æ ¸å¿ƒè¦ç‚¹ï¼Œå»é™¤å†—ä½™ä¿¡æ¯ã€‚

ç²¾ç®€è¦æ±‚ï¼š
1. ä¿ç•™å…³é”®ä¿¡æ¯
2. å»é™¤å†—ä½™æ­¥éª¤
3. ä½¿ç”¨ç®€æ´æ ¼å¼
4. ä¿æŒæ ¸å¿ƒé€»è¾‘

éœ€è¦ç²¾ç®€çš„å†…å®¹ï¼š`,
  } as const;
}

// åˆ›å»ºå•ä¾‹å®ä¾‹
export const openRouterService = new OpenRouterService();

export default OpenRouterService; 